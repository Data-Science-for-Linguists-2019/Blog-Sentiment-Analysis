{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report: [Blog-Sentiment Analysis](https://github.com/Data-Science-for-Linguists-2019/Blog-Sentiment-Analysis)\n",
    "\n",
    "To learn more about the data and see my previous analysis, refer to [Progress Report #2](https://github.com/Data-Science-for-Linguists-2019/Blog-Sentiment-Analysis/blob/master/progress_report_part2.ipynb).\n",
    "\n",
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/users/eva/Documents/Data_Science/Blog-Sentiment-Analysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogdata = pd.read_csv(dir + 'data/blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>industry</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age industry sign         date  \\\n",
       "0  2059027   male   15  Student  Leo  14,May,2004   \n",
       "1  2059027   male   15  Student  Leo  13,May,2004   \n",
       "2  2059027   male   15  Student  Leo  12,May,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcolumns = ['id', 'gender', 'age', 'industry', 'sign', 'date', 'text']\n",
    "blogdata.columns = newcolumns\n",
    "blogdata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "### Goals of my analysis\n",
    "There are three things I would like to investigate in this data. \n",
    "1. Word frequencies\n",
    "2. Blog topics\n",
    "3. Blog sentiment\n",
    "\n",
    "I already looked at [word frequencies](https://github.com/Data-Science-for-Linguists-2019/Blog-Sentiment-Analysis/blob/master/progress_report_part2.ipynb) in Progress Report #2, though I'm not super happy with my results. I experimented with [topic clustering using LDA (Latent Dirichlet Allocation) and scikit-learn](https://github.com/Data-Science-for-Linguists-2019/Blog-Sentiment-Analysis/blob/master/progress_report_part2.ipynb). \n",
    "\n",
    "For Progress Report #3, I am going to begin by exploring blog sentiment. If I have time, I will revisit the previous two goals of my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Ok so, it appears the vast majority of sentiment analysis groups texts into \"positive\" and \"negative\". I would really like to go deeper than that if possible, but I'll try positive and negative classification first.\n",
    "\n",
    "Since I am really not interested in labeling even a portion of the 681,284 blogs in this dataset for sentiment, I think I am going to try using some already trained models or wordlists with sentiment mapping. Many sentiment models were trained on traditional corpus data, using text from news and books. These models don't incorporate things like emoticons :-) and other internet-specific language which would be found in the blog dataset. In order to get the best sentiment judgments, I specifically looked for models trained on more modern corpora.\n",
    "\n",
    "### First try: VADER\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is \"a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media\". It is also part of NLTK, which is convenient. It looks like it can be used on a whole text without tokenization.\n",
    "Credit:\n",
    "> Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       well another days gone past and a major highlight of today has been two mass nose bleeds, while the blood is an itresting liquid its rather annoting when its streaning from your nose instead of someones neck. also today was the first full veiwing of the transit of venus across the sun whitnessed by mankind so though it may have been a pathetic black dot moving across the sun it was a once in a life time event and I'm slighty happyer that i've seen what some people can never see.  well with two nosebleeds, a mathes exam, a good roasting at the hands of the sun, an inability to focus properly and a ponding headache coming on I'm not feeling particly good tonight. but i've found my way back into hack this site which is a cool website which basicly gives the basics to the advanced on web hacking if your intrested its  urlLink hack-this-site dot org  aI'm quite happy with myself as im now twice as far as i got last time and i read loads of help articals then  well i dont feel like typing anything else tonight as my headaches getting realy bad but i hope anyone who reads this has a better evening than I'm going to and I'm sure you probably will  signing ooff one again its bald, goodnight people         \n"
     ]
    }
   ],
   "source": [
    "print(blogdata.text[30000])\n",
    "text = blogdata.text[30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some poking around I am going to try this blog - it seems pretty negative, but it also uses a lot of sarcasm and understatement, as well as mispellings, which might be an issue. And though it does focus on the negative it also mentions some happier things. Let's see how VADER classifies this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.052, 'neu': 0.798, 'pos': 0.149, 'compound': 0.9709}\n"
     ]
    }
   ],
   "source": [
    "nltk.sentiment.util.demo_vader_instance(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER ranks sentiment using a score between -1 and 1. Greater than 0.5 is positive, less than -0.5 is negative, and anything in between is neutral. `nltk.sentiment.util.demo_vader_instance` returns the percentage of the text considered negative, percentage considered neutral, percentage considered positive, and lastly the overall score for the document (`compound`). VADER did a not supergreat job, since it returned a very very positive score for a negative-seeming document, as well as categorizing most of the statements as \"neutral\". I considered this to be a pretty emotionally charged blog. Let's keep exploring VADER anyway and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"           I am pondering the idea of attempting to learn XML again, to make the site a bit more consistiant, in terms of tags, text sizes, and styles... Fear-not, it wont cut into the time I spen Dev'ing, just improve the work I do :)         \""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogdata.text[412412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.827, 'pos': 0.173, 'compound': 0.7727}\n"
     ]
    }
   ],
   "source": [
    "nltk.sentiment.util.demo_vader_instance(blogdata.text[412412])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement seems pretty darn neutral to me, though there is a smiley face. The classifier seems to lean positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Wat my feeling toward Sheffield??? Cold, cool and a bit of disappointment.   Cold – although now is summer, but the weather was quite cold especially the 1st day. It was a gloomy day and rain late of the day. The temperature was below 10 degree Celsius. Then the other days was like the weather in Genting Highlands and sometime even cooler then Genting… but I quite like the weather here, u can walk for hours with out any sweat in the afternoon. Believe it or not….  Cool – the cars, building, scenery and bla blab la were cool. After a few days here, I can see all my dream cars, Ferrari, Lambogini, Lotus, GTO, FTO, Impreza, BMW (the model tat drove by James Bond) and many many more tat I dunno the name. however, there is proton here also. Oh ya, kancil also got. Taken photo somemore… hahaha : p  oh.. bout the building, most of the building may seem a bit old, but inside the building was very nice and class leh…. E.g. Sheffield Hallam Uni… : )  Disappointment – cant get into the apartment type of accommodation as wat the told us earlier as we are the last batch… okie, dun mind bout it… but some of us (include me) was place in Marshall Hall (it is the name of the block and not a hall) for temporary as need to wait the full time student to move out from Broomgrove Hall – the hostel we suppose to stay in. tat mean cant unpack and buy all the necessities. Then there is not internet connection in our room.  Day 1 – reach the Manchester airport at bout 5.30am. but the sky was very bright liao, now summer mar, hav daylight from 5am till 10 or 11 pm. The airport is much smaller than KLIA…. Dun have much activities for the 1st day.. just that in the afternoon, our senior bring us to the Sheffield City and a Chinese groceries store… didn’t buy anything as still in temporary accommodation. … then most of the shop is close on Sunday. So there is very quiet and dun hav much people around the city. It was a tire day and sleep at 10 pm (I cant remember when is the last time I slept so early… \\uf04a) as I dun have any good sleep in the plane and bus…. Hehe…  Day 2 – wake up very early… bout 5 something in the morning… since my orientation will onli start at 9.30, so force myself into sleep again… hehe… I not pig lar, was still very tire leh…. The worst thing is we have to walk for 30-40 min to my uni. As usual the orientation start ed with a welcome speech, then registration, payment of fees and bla  bla bla…. Nothing much to say bout today….  Day 3 – hav a day off… but still hav to wake up early as need to go to the Hallam Union to get the ticket to Manchester and Bath (which it hav a limited seat).. will post up the photo after visiting those places… : )  After tat, as usual, shop for some necessities… we when to he market, I mean wet market. Just like in Malaysia, there is butcher, fish stalls, fruits…. But the diff is that their market r much more cleaner tat Malaysia n no bargaining….Not far from the market is the Peace Garden, a garden built to remember those warrior from Sheffield tat killed in wars. Many were  sun-bath there as today hav nice sunshine (which wont get this everyday)….. I was so happy leh today as Brian, Su Tsheng, Joyce, Rachel, Li Ping, Jo Anne, Cher Ching and Gim May have a simple celebration for my birthday… the 1st time celebrate my birthday at foreign country… hehehe …  \\uf04a  Day 4 – classes started today… so dun hav activities going on… just tat evening cook our own dinner… tis is the 1st time we have rice since we reached Sheffield…. So cham…. Hehehe… : p  Day 5 – finally we can move into Broomgrove Hall (remember??? The place we will stay for the rest of the time in Sheffield).. now can unpack all my belongings…. I was place in room 504,… okie very satisfy wif it, hav a nice scenery…. Hehehe…. Dun worry… will take photo also… hahahaha…  The is a television room… so watched the Euro 2004 match between England n Portugal… eh, wat happen 2 Beckam (missed 2 penalty kick liao), any so call Beckam fan can tell me… disappointed wif England performance (although I m not an England fan)… however the match was exciting leh….  Day 6 – nothing special… just attend lecture….at nite, we hav study grp…eh, wat wrong, 2day is fri nite n 1st week of class… I start doing my tutorial!!!  Something wrong wif me??? Okie.. I m still okie... just feel a bit weird… : ) … but feel good lar, atleast complete something… hehehe \\uf04a … fri nite mar, so after tat lepak at Brian’s room lar…  Day 7 – 9.45am woke up, should b the latest I woke up since reach Sheffield (not like me leh)…. : p  2day didn’t go for the trip to Warwick Castle, so wat the activities going on, okie, we when to Netto, the cheapest supermarket as told by the seniors…. They mayb right, the goods are quite cheap…so all of us buy quite some stuffs… it look like we torturing ourselves… we hav to carry our stuffs (which is heavy) all the way from Netto back to our room… which took bout 45 mins to 1 hour leh…. I think is the 1st n last time…          '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a longer text\n",
    "blogdata.text[300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.057, 'neu': 0.84, 'pos': 0.103, 'compound': 0.9921}\n"
     ]
    }
   ],
   "source": [
    "nltk.sentiment.util.demo_vader_instance(blogdata.text[300000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work pretty good on longer texts. I guess this text is positive overall? It's hard to tell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running VADER on 10k blog sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to explore some ways to extract information from sentiment analysis. I'm going to do this on a smaller sample of the data before trying them out on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = blogdata.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['lines'] = sample.text.map(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>industry</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608072</th>\n",
       "      <td>572994</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Religion</td>\n",
       "      <td>Libra</td>\n",
       "      <td>14,June,2004</td>\n",
       "      <td>whoever serves me must follo...</td>\n",
       "      <td>[                  whoever serves me must foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63959</th>\n",
       "      <td>3931176</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>25,July,2004</td>\n",
       "      <td>I hate my dad. God. Why is he so obsess...</td>\n",
       "      <td>[       I hate my dad., God., Why is he so obs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>873288</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>04,mars,2004</td>\n",
       "      <td>I just tried to post a monster blog...</td>\n",
       "      <td>[           I just tried to post a monster blo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age  industry         sign          date  \\\n",
       "608072   572994    male   24  Religion        Libra  14,June,2004   \n",
       "63959   3931176  female   14    indUnk  Sagittarius  25,July,2004   \n",
       "10690    873288  female   23      Arts    Capricorn  04,mars,2004   \n",
       "\n",
       "                                                     text  \\\n",
       "608072                    whoever serves me must follo...   \n",
       "63959          I hate my dad. God. Why is he so obsess...   \n",
       "10690              I just tried to post a monster blog...   \n",
       "\n",
       "                                                    lines  \n",
       "608072  [                  whoever serves me must foll...  \n",
       "63959   [       I hate my dad., God., Why is he so obs...  \n",
       "10690   [           I just tried to post a monster blo...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sia.polarity_scores(sample['text'][608072])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sentiment(df):\n",
    "    all_scores = []\n",
    "    for x in df.index:\n",
    "        score = sia.polarity_scores(df['text'][x])\n",
    "        if score['compound'] >= 0.5:\n",
    "            all_scores.append('positive')\n",
    "        if score['compound'] <= -0.5:\n",
    "            all_scores.append('negative')\n",
    "        else:\n",
    "            all_scores.append('neutral')\n",
    "    return all_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15436"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d0668400b858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "sample['sentiment'] = summarize_sentiment(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try: AFINN\n",
    "[AFINN](https://github.com/fnielsen/afinn) is described as a \"Wordlist-based approach for sentiment analysis\". The wordlist was created using data from Twitter. It contains basically every emoticon and things like \"wowow\" (4/5 on the positive scale) and \"wtfff\" (-4/-5 on the negative scale). I read the paper accompanying the wordlist and thought it sounded like a potential fit for the blog data.\n",
    "\n",
    "> Finn Årup Nielsen, \"A new ANEW: evaluation of a word list for sentiment analysis in microblogs\", Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages. Volume 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
